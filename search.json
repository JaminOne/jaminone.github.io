[{"title":"Web Scraping Raywhite NZ to get Auckland House Data","url":"/2020/07/28/Web-Scraping-Raywhite-to-get-House-Data/","content":"G-Day! It's a windy Tuesday, but we still need to keep moving, right? Today I will share a web scraping experience by using scrapy Python framework, and I will scrape the Auckland house data from Raywhite, which could be used for future data analysis. Firstly, a huge shout out to Ben who helped me a lot in API and web scraping task at Microsoft Student Accelerator NZ. I will talk about my Data Science and Machine Learning phase 1 challenge in my next blog. Alrighty, without further ado, let's jump to the topic.\n\nThe preparation is quite simple. I created a new environment at Anaconda by installing **Scrapy and Regex**, and I use the VS code to fulfil this task.\n\nFirstly, I generated the scrapy project by using the command line:\n'scrapy genspider rw raywhite.co.nz'\nrw is the project name and followed by an initial url. Then go the spiders file and open the rw.py file. Note that it's better to replace the initial url with the real url we would love to do web scraping. For example, the link with applying searching condition, special sub-page, etc.\n\nThere are two command lines which are useful in this task\n**Run the scrapy file: **\n`scrapy crawl name` \n**Create csv file with the scraping data: **\n`scrapy crawl name -o name.csv`\n\nThe body of the scraping code is a **parse** function. Firstly, we need to find the iterative part of the page. In the Raywhite example, it's the \"Card\". A small chick to use \"Element selection\" feature to locate the related code on Chrome.\n\n![chromefeature](https://raw.githubusercontent.com/JaminOne/jaminone.github.io/master/img/locate%20the%20area.jpg)\n\nTo get all the content in each element, there is a very useful code that we should all remember:\n`response.xpath('//div[@   ]')`\n\nPython's string manipulation is crucial in web scraping. In this task, I use strip(), replace() and join() to fulfil the objective. The code is below:\n\n`address = card.xpath('./a/h5/text()').getall()`\n`address = [x.strip() for x in address]`\n`address = [x.replace('\\xa0',' ') for x in address]`\n`address = \", \".join(address)`\n\nIn addition, Regular Expressions is often used in webscraping in order to find the pattern of number+character combination. \n\nThis is a great resource! - [https://www.datacamp.com/community/tutorials/python-regular-expression-tutorial](https://www.datacamp.com/community/tutorials/python-regular-expression-tutorial)\n\nOtherwise here is a crash course!\n\n\".\" matches anything, ie. wildcard\n[0-9] matches a number\n[a-z] matches a lower case letter\n[A-Z] matches an upper case letter\n[a-zA-Z] matches an any letter\n[a-zA-Z0-9] matches any alphanumeric character\n\n1.? means 0 or 1 instance. So [0-9]? indicates a pattern where there is 0 or 1 number\n2.+ means 1 or more instance. So [0-9]+ indicates a pattern where there is 1 or more number(s)\n3.* means 0 or more instance. So [A-Z]* indicates a pattern where there is 0 or more upper case letters\n\n\n\n\n","tags":["Web Scraping, Python, Scrapy"]},{"title":"AWS Design Patterns and Sample Architectures","url":"/2020/07/16/Design-Patterns-and-Sample-Architectures/","content":"**About high availability**\nFirstly and most importantly, secure HA by using the multi-AZ pattern.\n1. AMI \n2. Load balancer \n3. confirm the healthy state\n4. Create master and standby RDS read replica for automatic failover\n5. Use elastic Ip address. Disassociate the Elastic IP address from the original one, and associate with the new one. Easy fallback procedure.\n6. Stateless (ElastiCache, DynamoDB, store in the data store rather than web/app server)\n7. Scaling by schedule or by policy\n8. Use Amazon SQS to do the batch job\n9. Bootstrap\n10. bootstrap instance\n\n![](https://raw.githubusercontent.com/JaminOne/jaminone.github.io/master/img/AWSassociatearchitecture.png)\n**Recommended service for big-data application architecture**\nAWS Associates Programme uses Amazon EMR with hadoop to remove performance bottleneck cause by the single-threaded C++ application, and we can also add analytic and data insight with Amazon Redshift.\n\n**Batch processing vs stream process**\nDelay vs real time. It is recommended to use the later one. Kinesis is the perfect service for that.\nEx: Twitter Trend\n![](https://raw.githubusercontent.com/JaminOne/jaminone.github.io/master/img/twittertrend.jpg)\n\n**Case study: COVID19 Outbreak heatmap**\nTraditional method: Regional governors send data to the FTP server. It's highly like to cause single point of failure. Aggregating data is delay and slow. AWS Kinesis can generate real time data but the payload size is limited. The global data may exceed the limit. Cloudfrond is compatible with this case, but it doesn't cache the data. There are lots of tradeoff when you look into this case.\n\n"},{"title":"Project: OpenCV with Python to deal with image detection and process","url":"/2020/07/10/Project-OpenCV-with-Python-to-deal-with-image-detection-and-process/","content":"OpenCV is a powerful library in python when the objects you dealing are images and videos. Before the project starts, I need to install opencv-python, Numpy and Matplotlib in command line.\n```\npip install numpy\npip install matplotlib\npip install opencv-python\n```\nOpenCV's official [documentation page](https://docs.opencv.org/4.3.0/d2/d96/tutorial_py_table_of_contents_imgproc.html) \n\nPlease note that OpenCV is not compatible with jupyter notebook and spyder, therefore I used Visual Studio to do this task.\n\nThe final result can circle each coin with its value, and calculate total value on the image as well.\n![](https://raw.githubusercontent.com/JaminOne/jaminone.github.io/master/img/710opencv.jpg)\n\nMy code is below:\n```\nimport numpy as np\nimport cv2\nfrom matplotlib import pyplot as plt\n\n#function \ndef get_radius(circles):\n    radius = []\n    for j in circles[0,:]:\n        radius.append(j[2])\n    return radius\n\ndef av_pix(img,circles,size):\n    av_value = []\n    for coords in circles[0,:]:\n        col = np.mean(img[coords[1]-size:coords[1]+size,coords[0]-size:coords[0]+size])\n        #print(img[coords[1]-size:coords[1]+size,coords[0]-size:coords[0]+size])\n        av_value.append(col)\n    return av_value  \n\nimg = cv2.imread('E:\\\\NZStudy\\\\Python\\\\capstone_coins.png',0)\noriginal_image = cv2.imread('E:\\\\NZStudy\\\\Python\\\\capstone_coins.png',1)\n# convert BGR to RGB to be suitable for showing using matplotlib library\nimg = cv2.medianBlur(img,5)\ncimg = cv2.cvtColor(img,cv2.COLOR_GRAY2BGR)\nfont = cv2.FONT_HERSHEY_SIMPLEX \n\n#use houghcircles to detect coin\ncircles = cv2.HoughCircles(img,cv2.HOUGH_GRADIENT,1,130,\n                            param1=44,param2=75,minRadius=0,maxRadius=200)\nprint(circles)\n\ncircles = np.uint16(np.around(circles))\ncount = 1\nfor i in circles[0,:]:\n    # draw the outer circle\n    cv2.circle(original_image,(i[0],i[1]),i[2],(0,255,0),2)\n    # draw the center of the circle\n    cv2.circle(original_image,(i[0],i[1]),2,(0,0,255),3)\n    count +=1\n\n\nradii = get_radius(circles)\nprint(radii)\n\nbright_values = av_pix(img,circles,20)\nprint(bright_values)\n\nvalues = []\nfor a,b in zip(bright_values,radii):\n    if a > 150 and b > 106.5:\n        values.append(10)\n    elif a > 150 and b <= 106.5:\n        values.append(5)\n    elif a < 150 and b > 106.5:\n        values.append(2)\n    elif a < 150 and b < 106.5:\n        values.append(1)        \nprint(values)           \ncount_2 = 0\nfor i in circles[0,:]:\n    \n    cv2.putText(original_image, str(values[count_2]) + 'p',(i[0],i[1]), cv2.FONT_HERSHEY_SIMPLEX, 2, (0,0,0), 2)\n    count_2 += 1\ncv2.putText(original_image, 'ESTIMATED TOTAL VALUE: ' + str(sum(values)) + 'p', (200,100), cv2.FONT_HERSHEY_SIMPLEX, 1.3, 255)\n \noriginal_image = cv2.resize(original_image, (960, 540))\ncv2.imshow('detected circles',original_image)\ncv2.waitKey(0)\ncv2.destroyAllWindows()\n```"}]