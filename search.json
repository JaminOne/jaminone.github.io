[{"title":"MSA Phase 1 - Use Linear Regression to Predict Auckland’s House Prices","url":"/2020/07/31/Use-Linear-Regression-to-Predict-Aucklands-House-Prices/","content":"## Executive Summary\n\nProvided by MSA team, the original dataset is about house price in Auckland, and two additional columns have been added by joining with the datasets from Stat.NZ and the University of Otago respectively, which are both reliable sources. It not only contains the basic information of the property such as land area and the number of rooms but also includes population data from census in 2018. \n\nThe analysis is based on 1051 observations for each of the 17 variables. The most important attribute is the value of the property (CV), which is an approximation of value I am going to predict in the data analysis. The main house and population data is intuitive, and all of them have statistic measurement such as mean, min, max, standard error, etc. Besides, there are several geographical data that could be used to enhance visualization, providing a better interpretation for the decision-maker. However, it could be redundant because when the dataset has Suburbs, Latitude, and Longitude variables, Address becomes unnecessary, and it can be dropped in the modelling phase. Lastly, the dataset also contains the deprivation index data. It’s considered as a valuable attribute since it can be utilized for the social psychology study, and in this case, more related data needs to be collected to fulfil the task.\n\nThis report has been divided into three different parts. Firstly, I will explore the data by calculating summary and descriptive statistics, following by the visualization of the correlation between the numerical variable and geographic data by grouping different suburbs within the same value range. Finally, three algorithms have been tested for the training dataset, and the best model has been selected based on the model’s accuracy.     \n\n## Explore the Initial Data\n\nThe count, mean, min and max rows are self-explanatory. The std shows the standard deviation, and the 25%, 50% and 75% rows show the corresponding percentiles. Although there are some houses with 17 bedrooms or 8 bathrooms, I still believe that they are not outliers as it could be the luxury house or for tourist rental purpose. \n \n \n\nTo get a better feel of what kind of data I am dealing with, I plot a histogram for each numeric variable. Some of the histograms are left-skewed because for the SA1 unit date, it ideally has a size range of 100-200 residents.\n \n## Explore the Price with Suburb Data\n\nSince each house has a different land area and each suburb has different home prices, I add a new price per square meter variable correlates with house value. Then I can use price per square meter’s range to get clusters of suburbs. \n \n\n## Correlation and Relationships\n\nThe correlation between the numeric columns has been plotted. The colour and digit numbers on the cell indicate the correlation values which are between -1 and 1. The graph shows that for the house price, the individual variable has little positive correlation, but it could be different when combining multiple variables. The cost of the house increases when the number of bathrooms goes up. In the meantime, you can also see some negative correlation. Coefficients close to zero indicate that there is no linear correlation.\n \n \n\n## Geographic Data Visualization\n\nTo create a better data visualization, I create a scatter plot with latitude and longitude.\n \nI used colour code from the most expensive to the least expensive areas. If I can add an Auckland map layer, it could be more intuitive.\n \n\n## Analysis\n\nThere are three machine learning models been tested to predict the house price, namely Linear Regression, Random Forest Regression and Gradient boosting, and the performance of each model is below. These algorithms were trained with 70% of the data. Testing the model with the remaining 30% of the data.\n\nName\tR squared\tRMSE\nLinear Regression\t0.4365\t1978728.8932\nRandom Forest Regression\t0.5728\t1978728.8477\nGradient boosting\t0.5903\t1978728.8512\n\nAs we can see from the table, Gradient Boosting has the highest accuracy, and the model predicts the value of the property is around NZD1,978,728.\n\nFrom the feature importance table, we can see which features are more important in Gradient Boosting model, and the top 3 features are NZDep2018, SA1, Land Area. \n \n\n## Conclusion\n\nThe analysis shows that the house price can be predicted by current dataset and variables, but there is still large space to improve because the best model’s accuracy is less than 60%. In my opinion, the population data is not usually related to the price. Suppose we could change population data to more house info related data such as type, house of the year, facilities, etc. It could get a better result.\n","tags":["Linear Regression"]},{"title":"Web Scraping Raywhite NZ to get Auckland House Data","url":"/2020/07/28/Web-Scraping-Raywhite-to-get-House-Data/","content":"G-Day! It's a windy Tuesday, but we still need to keep moving, right? Today I will share a web scraping experience by using scrapy Python framework, and I will scrape the Auckland house data from Raywhite, which could be used for future data analysis. Firstly, a huge shout out to Ben who helped me a lot in API and web scraping task at Microsoft Student Accelerator NZ. I will talk about my Data Science and Machine Learning phase 1 challenge in my next blog. Alrighty, without further ado, let's jump to the topic.\n\nThe preparation is quite simple. I created a new environment at Anaconda by installing **Scrapy and Regex**, and I use the VS code to fulfil this task.\n\nFirstly, I generated the scrapy project by using the command line:\n'scrapy genspider rw raywhite.co.nz'\nrw is the project name and followed by an initial url. Then go the spiders file and open the rw.py file. Note that it's better to replace the initial url with the real url we would love to do web scraping. For example, the link with applying searching condition, special sub-page, etc.\n\nThere are two command lines which are useful in this task\n**Run the scrapy file: **\n`scrapy crawl name` \n**Create csv file with the scraping data: **\n`scrapy crawl name -o name.csv`\n\nThe body of the scraping code is a **parse** function. Firstly, we need to find the iterative part of the page. In the Raywhite example, it's the \"Card\". A small chick to use \"Element selection\" feature to locate the related code on Chrome.\n\n![chromefeature](https://raw.githubusercontent.com/JaminOne/jaminone.github.io/master/img/locate%20the%20area.jpg)\n\nTo get all the content in each element, there is a very useful code that we should all remember:\n`response.xpath('//div[@   ]')`\n\nPython's string manipulation is crucial in web scraping. In this task, I use strip(), replace() and join() to fulfil the objective. The code is below:\n\n`address = card.xpath('./a/h5/text()').getall()`\n`address = [x.strip() for x in address]`\n`address = [x.replace('\\xa0',' ') for x in address]`\n`address = \", \".join(address)`\n\nIn addition, Regular Expressions is often used in webscraping in order to find the pattern of number+character combination. \n\nThis is a great resource! - [https://www.datacamp.com/community/tutorials/python-regular-expression-tutorial](https://www.datacamp.com/community/tutorials/python-regular-expression-tutorial)\n\nOtherwise here is a crash course!\n\n\".\" matches anything, ie. wildcard\n[0-9] matches a number\n[a-z] matches a lower case letter\n[A-Z] matches an upper case letter\n[a-zA-Z] matches an any letter\n[a-zA-Z0-9] matches any alphanumeric character\n\n1.? means 0 or 1 instance. So [0-9]? indicates a pattern where there is 0 or 1 number\n2.+ means 1 or more instance. So [0-9]+ indicates a pattern where there is 1 or more number(s)\n3.* means 0 or more instance. So [A-Z]* indicates a pattern where there is 0 or more upper case letters\n\n\n\n\n","tags":["Scrapy"]},{"title":"AWS Design Patterns and Sample Architectures","url":"/2020/07/16/Design-Patterns-and-Sample-Architectures/","content":"**About high availability**\nFirstly and most importantly, secure HA by using the multi-AZ pattern.\n1. AMI \n2. Load balancer \n3. confirm the healthy state\n4. Create master and standby RDS read replica for automatic failover\n5. Use elastic Ip address. Disassociate the Elastic IP address from the original one, and associate with the new one. Easy fallback procedure.\n6. Stateless (ElastiCache, DynamoDB, store in the data store rather than web/app server)\n7. Scaling by schedule or by policy\n8. Use Amazon SQS to do the batch job\n9. Bootstrap\n10. bootstrap instance\n\n![](https://raw.githubusercontent.com/JaminOne/jaminone.github.io/master/img/AWSassociatearchitecture.png)\n**Recommended service for big-data application architecture**\nAWS Associates Programme uses Amazon EMR with hadoop to remove performance bottleneck cause by the single-threaded C++ application, and we can also add analytic and data insight with Amazon Redshift.\n\n**Batch processing vs stream process**\nDelay vs real time. It is recommended to use the later one. Kinesis is the perfect service for that.\nEx: Twitter Trend\n![](https://raw.githubusercontent.com/JaminOne/jaminone.github.io/master/img/twittertrend.jpg)\n\n**Case study: COVID19 Outbreak heatmap**\nTraditional method: Regional governors send data to the FTP server. It's highly like to cause single point of failure. Aggregating data is delay and slow. AWS Kinesis can generate real time data but the payload size is limited. The global data may exceed the limit. Cloudfrond is compatible with this case, but it doesn't cache the data. There are lots of tradeoff when you look into this case.\n\n","tags":["ACA"]},{"title":"Project: OpenCV with Python to deal with image detection and process","url":"/2020/07/10/Project-OpenCV-with-Python-to-deal-with-image-detection-and-process/","content":"OpenCV is a powerful library in python when the objects you dealing are images and videos. Before the project starts, I need to install opencv-python, Numpy and Matplotlib in command line.\n```\npip install numpy\npip install matplotlib\npip install opencv-python\n```\nOpenCV's official [documentation page](https://docs.opencv.org/4.3.0/d2/d96/tutorial_py_table_of_contents_imgproc.html) \n\nPlease note that OpenCV is not compatible with jupyter notebook and spyder, therefore I used Visual Studio to do this task.\n\nThe final result can circle each coin with its value, and calculate total value on the image as well.\n![](https://raw.githubusercontent.com/JaminOne/jaminone.github.io/master/img/710opencv.jpg)\n\nMy code is below:\n```\nimport numpy as np\nimport cv2\nfrom matplotlib import pyplot as plt\n\n#function \ndef get_radius(circles):\n    radius = []\n    for j in circles[0,:]:\n        radius.append(j[2])\n    return radius\n\ndef av_pix(img,circles,size):\n    av_value = []\n    for coords in circles[0,:]:\n        col = np.mean(img[coords[1]-size:coords[1]+size,coords[0]-size:coords[0]+size])\n        #print(img[coords[1]-size:coords[1]+size,coords[0]-size:coords[0]+size])\n        av_value.append(col)\n    return av_value  \n\nimg = cv2.imread('E:\\\\NZStudy\\\\Python\\\\capstone_coins.png',0)\noriginal_image = cv2.imread('E:\\\\NZStudy\\\\Python\\\\capstone_coins.png',1)\n# convert BGR to RGB to be suitable for showing using matplotlib library\nimg = cv2.medianBlur(img,5)\ncimg = cv2.cvtColor(img,cv2.COLOR_GRAY2BGR)\nfont = cv2.FONT_HERSHEY_SIMPLEX \n\n#use houghcircles to detect coin\ncircles = cv2.HoughCircles(img,cv2.HOUGH_GRADIENT,1,130,\n                            param1=44,param2=75,minRadius=0,maxRadius=200)\nprint(circles)\n\ncircles = np.uint16(np.around(circles))\ncount = 1\nfor i in circles[0,:]:\n    # draw the outer circle\n    cv2.circle(original_image,(i[0],i[1]),i[2],(0,255,0),2)\n    # draw the center of the circle\n    cv2.circle(original_image,(i[0],i[1]),2,(0,0,255),3)\n    count +=1\n\n\nradii = get_radius(circles)\nprint(radii)\n\nbright_values = av_pix(img,circles,20)\nprint(bright_values)\n\nvalues = []\nfor a,b in zip(bright_values,radii):\n    if a > 150 and b > 106.5:\n        values.append(10)\n    elif a > 150 and b <= 106.5:\n        values.append(5)\n    elif a < 150 and b > 106.5:\n        values.append(2)\n    elif a < 150 and b < 106.5:\n        values.append(1)        \nprint(values)           \ncount_2 = 0\nfor i in circles[0,:]:\n    \n    cv2.putText(original_image, str(values[count_2]) + 'p',(i[0],i[1]), cv2.FONT_HERSHEY_SIMPLEX, 2, (0,0,0), 2)\n    count_2 += 1\ncv2.putText(original_image, 'ESTIMATED TOTAL VALUE: ' + str(sum(values)) + 'p', (200,100), cv2.FONT_HERSHEY_SIMPLEX, 1.3, 255)\n \noriginal_image = cv2.resize(original_image, (960, 540))\ncv2.imshow('detected circles',original_image)\ncv2.waitKey(0)\ncv2.destroyAllWindows()\n```","tags":["Machine Learning"]}]