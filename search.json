[{"title":"Practice: Plot the Space Station's Location in Real Time","url":"/2020/08/25/Find-Space-Station/","content":"Happy birthday to myself! It has been a tough year and I am glad my study goes well. Today, let's take a look at an easy task by using  API and Plotly to capture the International Spcade Station(ISS) location in the real time. By using the while loop and Time library, we can fetch the data in certain period of time. There are some key takeaways listed below.\n\n- When convert the dtype to datetime, the 10 digit unix code can be transfer by **adding unit ='s'**\n- A good way to loop through pandas dataframe is appending it into a list first and concat the whole list to dataframe.\n- Sometimes read_json can replace Requests + json + pd.DataFrame.from_dict\n- Plotly is a great tool to visualise the geo data. When I ploted the world map, the ISS just passed New Zealand. What a coincidence! \n\n![Plotly](https://raw.githubusercontent.com/JaminOne/hexo-aircloud-blog/master/source/img/plotly.png)\n\nCode:\n```\n#import necessary libraries\nimport pandas as pd\nimport json\nimport datetime\nimport requests\nimport plotly.express as px\nimport time\n\n#visit the API in the certain period and get the data\ni = 0\ngeo_list = [] \nwhile i<10:\n    r = requests.get('http://api.open-notify.org/iss-now.json')\n    df_dic = r.json()\n    df = pd.DataFrame.from_dict(df_dic)\n    df['latitude'] = df.loc['latitude','iss_position']\n    df['longitude'] = df.loc['longitude','iss_position']\n    df.reset_index(inplace=True)\n    df['timestamp'] = pd.to_datetime(df['timestamp'],unit='s')\n    df = df.drop(['message','index','iss_position'],axis=1)\n    df = df.drop([1])\n    geo_list.append(df)\n    time.sleep(50)\n    i+=1\ngeo_df = pd.concat(geo_list)\n\n#plotly catter plot\nfig = px.scatter_geo(geo_df,lat='latitude',lon='longitude')\nfig.show()\n\n```","tags":["Pandas"]},{"title":"AWS Cloud Solution Architect Exam Preparation 1","url":"/2020/08/03/AWS-Cloud-Solution-Architect-exam-preparation/","content":"I recently started to prepare my AWS ACA exam, and I found out that the official modules I have learnt were way easier comparing to practice exam questions. But anyway, I will get there, and in following posts, I will share some learning points and thoughts I got from the practice exam.\n1. ElastiCache for Redis\nElastiCache is a fully managed in-memory data store and cache service. ElastiCache supports two open-source in-memory caching engines: Memcached and Redis. \nRedis, which stands for Remote Dictionary Server, is a fast, open-source, in-memory key-value data store for use as a database, cache, message broker, and queue. \n\n2. ElastiCache in-transit or at-rest encryption:  you must meet the following conditions.\n\nYour cluster or replication group must be running Redis 3.2.6, 4.0.10 or later.\nYour cluster or replication group must be created in a VPC based on Amazon VPC.\nOptionally, you can also use AUTH and the AUTH token (password) needed to perform operations on this cluster or replication group.\n\n3.  Temporary credentials in AWS\n- Setup a Federation proxy or an Identity provider\n- Setup an AWS Security Token Service to generate temporary tokens\n- Configure an IAM role and an IAM Policy to access the bucket.\n\n4.  increase the write performance of the database hosted in an EC2 instance\n- increasing the size of the EC2 instance.\n- set up a standard RAID 0 configuration \nall RAID is accomplished at the software level. For greater I/O performance than you can achieve with a single volume, RAID 0 can stripe multiple volumes together; \n\n5. HVM AMI vs PV AMI\nLinux Amazon Machine Images use one of two types of virtualization: paravirtual (PV) or hardware virtual machine (HVM). The main differences between PV and HVM AMIs are the way in which they boot and whether they can take advantage of special hardware extensions (CPU, network, and storage) for better performance.\n\n6. RAID 1 vs RAID 0\nUsing a standard RAID 1 configuration with two EBS Volumes is incorrect because the main use case for RAID 1 is to provide mirroring, redundancy, and fault-tolerance. RAID 0 is a more suitable option for providing faster read and write operations, compared with RAID 1.\n\n7. S3 Pre-signed URL\nobject owner can optionally share objects with others by creating a pre-signed URL, using their own security credentials, to grant time-limited permission to download the objects.\n\n8. Metrics not defaultly available in CloudWatch \ncertain metrics that are not readily available in CloudWatch such as memory utilization, disk space utilization, and many others which can be collected by setting up a custom metric.\n- Memory utilization\n- Disk swap utilization\n- Disk space utilization\n- Page file utilization\n- Log collection\n\n9. Auto Scaling group termination policy\n- If there are instances in multiple Availability Zones, choose the Availability Zone with the most instances and at least one instance that is not protected from scale in. If there is more than one Availability Zone with this number of instances, choose the Availability Zone with the instances that use the oldest launch configuration.\n- Determine which unprotected instances in the selected Availability Zone use the oldest launch configuration. If there is one such instance, terminate it.\n- If there are multiple instances to terminate based on the above criteria, determine which unprotected instances are closest to the next billing hour. (This helps you maximize the use of your EC2 instances and manage your Amazon EC2 usage costs.) If there is one such instance, terminate it.\n- If there is more than one unprotected instance closest to the next billing hour, choose one of these instances at random.\n\n10. How to avoid accidental deletion in Amazon S3 bucket\n- Enable Versioning\n- Enable MFA (Multi-Factor Authentication) Delete","tags":["ACA"]},{"title":"MSA Phase 1 - Use Linear Regression to Predict Auckland’s House Prices","url":"/2020/07/31/Use-Linear-Regression-to-Predict-Aucklands-House-Prices/","content":"## Executive Summary\n\nProvided by MSA team, the original dataset is about house price in Auckland, and two additional columns have been added by joining with the datasets from Stat.NZ and the University of Otago respectively, which are both reliable sources. It not only contains the basic information of the property such as land area and the number of rooms but also includes population data from census in 2018. \n\nThe analysis is based on 1051 observations for each of the 17 variables. The most important attribute is the value of the property (CV), which is an approximation of value I am going to predict in the data analysis. The main house and population data is intuitive, and all of them have statistic measurement such as mean, min, max, standard error, etc. Besides, there are several geographical data that could be used to enhance visualization, providing a better interpretation for the decision-maker. However, it could be redundant because when the dataset has Suburbs, Latitude, and Longitude variables, Address becomes unnecessary, and it can be dropped in the modelling phase. Lastly, the dataset also contains the deprivation index data. It’s considered as a valuable attribute since it can be utilized for the social psychology study, and in this case, more related data needs to be collected to fulfil the task.\n\nThis report has been divided into three different parts. Firstly, I will explore the data by calculating summary and descriptive statistics, following by the visualization of the correlation between the numerical variable and geographic data by grouping different suburbs within the same value range. Finally, three algorithms have been tested for the training dataset, and the best model has been selected based on the model’s accuracy.     \n\n## Explore the Initial Data\n\nThe count, mean, min and max rows are self-explanatory. The std shows the standard deviation, and the 25%, 50% and 75% rows show the corresponding percentiles. Although there are some houses with 17 bedrooms or 8 bathrooms, I still believe that they are not outliers as it could be the luxury house or for tourist rental purpose. \n \n![initialdata](https://raw.githubusercontent.com/JaminOne/jaminone.github.io/master/img/731image002.jpg) \n![initialdata2](https://raw.githubusercontent.com/JaminOne/jaminone.github.io/master/img/731image004.jpg) \n\nTo get a better feel of what kind of data I am dealing with, I plot a histogram for each numeric variable. Some of the histograms are left-skewed because for the SA1 unit date, it ideally has a size range of 100-200 residents.\n\n![histogram](https://raw.githubusercontent.com/JaminOne/jaminone.github.io/master/img/731image006.jpg) \n \n## Explore the Price with Suburb Data\n\nSince each house has a different land area and each suburb has different home prices, I add a new price per square meter variable correlates with house value. Then I can use price per square meter’s range to get clusters of suburbs. \n \n![suburb](https://raw.githubusercontent.com/JaminOne/jaminone.github.io/master/img/731image008.jpg) \n\n## Correlation and Relationships\n\nThe correlation between the numeric columns has been plotted. The colour and digit numbers on the cell indicate the correlation values which are between -1 and 1. The graph shows that for the house price, the individual variable has little positive correlation, but it could be different when combining multiple variables. The cost of the house increases when the number of bathrooms goes up. In the meantime, you can also see some negative correlation. Coefficients close to zero indicate that there is no linear correlation.\n \n![correlation](https://raw.githubusercontent.com/JaminOne/jaminone.github.io/master/img/731image010.jpg)  \n![correlation2](https://raw.githubusercontent.com/JaminOne/jaminone.github.io/master/img/731image011.png)  \n\n## Geographic Data Visualization\n\nTo create a better data visualization, I create a scatter plot with latitude and longitude.\n\n![visualization](https://raw.githubusercontent.com/JaminOne/jaminone.github.io/master/img/731image013.png)  \n \nI used colour code from the most expensive to the least expensive areas. If I can add an Auckland map layer, it could be more intuitive.\n \n![visualization2](https://raw.githubusercontent.com/JaminOne/jaminone.github.io/master/img/731image015.png)  \n\n## Analysis\n\nThere are three machine learning models been tested to predict the house price, namely Linear Regression, Random Forest Regression and Gradient boosting, and the performance of each model is below. These algorithms were trained with 70% of the data. Testing the model with the remaining 30% of the data.\n\nName\tR squared\tRMSE\nLinear Regression\t0.4365\t1978728.8932\nRandom Forest Regression\t0.5728\t1978728.8477\nGradient boosting\t0.5903\t1978728.8512\n\nAs we can see from the table, Gradient Boosting has the highest accuracy, and the model predicts the value of the property is around NZD1,978,728.\n\nFrom the feature importance table, we can see which features are more important in Gradient Boosting model, and the top 3 features are NZDep2018, SA1, Land Area. \n \n![feature](https://raw.githubusercontent.com/JaminOne/jaminone.github.io/master/img/731image016.png)  \n\n## Conclusion\n\nThe analysis shows that the house price can be predicted by current dataset and variables, but there is still large space to improve because the best model’s accuracy is less than 60%. In my opinion, the population data is not usually related to the price. Suppose we could change population data to more house info related data such as type, house of the year, facilities, etc. It could get a better result.\n","tags":["Linear Regression"]},{"title":"Web Scraping Raywhite NZ to get Auckland House Data","url":"/2020/07/28/Web-Scraping-Raywhite-to-get-House-Data/","content":"G-Day! It's a windy Tuesday, but we still need to keep moving, right? Today I will share a web scraping experience by using scrapy Python framework, and I will scrape the Auckland house data from Raywhite, which could be used for future data analysis. Firstly, a huge shout out to Ben who helped me a lot in API and web scraping task at Microsoft Student Accelerator NZ. I will talk about my Data Science and Machine Learning phase 1 challenge in my next blog. Alrighty, without further ado, let's jump to the topic.\n\nThe preparation is quite simple. I created a new environment at Anaconda by installing **Scrapy and Regex**, and I use the VS code to fulfil this task.\n\nFirstly, I generated the scrapy project by using the command line:\n'scrapy genspider rw raywhite.co.nz'\nrw is the project name and followed by an initial url. Then go the spiders file and open the rw.py file. Note that it's better to replace the initial url with the real url we would love to do web scraping. For example, the link with applying searching condition, special sub-page, etc.\n\nThere are two command lines which are useful in this task\n**Run the scrapy file: **\n`scrapy crawl name` \n**Create csv file with the scraping data: **\n`scrapy crawl name -o name.csv`\n\nThe body of the scraping code is a **parse** function. Firstly, we need to find the iterative part of the page. In the Raywhite example, it's the \"Card\". A small chick to use \"Element selection\" feature to locate the related code on Chrome.\n\n![chromefeature](https://raw.githubusercontent.com/JaminOne/jaminone.github.io/master/img/locate%20the%20area.jpg)\n\nTo get all the content in each element, there is a very useful code that we should all remember:\n`response.xpath('//div[@   ]')`\n\nPython's string manipulation is crucial in web scraping. In this task, I use strip(), replace() and join() to fulfil the objective. The code is below:\n\n`address = card.xpath('./a/h5/text()').getall()`\n`address = [x.strip() for x in address]`\n`address = [x.replace('\\xa0',' ') for x in address]`\n`address = \", \".join(address)`\n\nIn addition, Regular Expressions is often used in webscraping in order to find the pattern of number+character combination. \n\nThis is a great resource! - [https://www.datacamp.com/community/tutorials/python-regular-expression-tutorial](https://www.datacamp.com/community/tutorials/python-regular-expression-tutorial)\n\nOtherwise here is a crash course!\n\n\".\" matches anything, ie. wildcard\n[0-9] matches a number\n[a-z] matches a lower case letter\n[A-Z] matches an upper case letter\n[a-zA-Z] matches an any letter\n[a-zA-Z0-9] matches any alphanumeric character\n\n1.? means 0 or 1 instance. So [0-9]? indicates a pattern where there is 0 or 1 number\n2.+ means 1 or more instance. So [0-9]+ indicates a pattern where there is 1 or more number(s)\n3.* means 0 or more instance. So [A-Z]* indicates a pattern where there is 0 or more upper case letters\n\n\n\n\n","tags":["Scrapy"]},{"title":"AWS Design Patterns and Sample Architectures","url":"/2020/07/16/Design-Patterns-and-Sample-Architectures/","content":"**About high availability**\nFirstly and most importantly, secure HA by using the multi-AZ pattern.\n1. AMI \n2. Load balancer \n3. confirm the healthy state\n4. Create master and standby RDS read replica for automatic failover\n5. Use elastic Ip address. Disassociate the Elastic IP address from the original one, and associate with the new one. Easy fallback procedure.\n6. Stateless (ElastiCache, DynamoDB, store in the data store rather than web/app server)\n7. Scaling by schedule or by policy\n8. Use Amazon SQS to do the batch job\n9. Bootstrap\n10. bootstrap instance\n\n![](https://raw.githubusercontent.com/JaminOne/jaminone.github.io/master/img/AWSassociatearchitecture.png)\n**Recommended service for big-data application architecture**\nAWS Associates Programme uses Amazon EMR with hadoop to remove performance bottleneck cause by the single-threaded C++ application, and we can also add analytic and data insight with Amazon Redshift.\n\n**Batch processing vs stream process**\nDelay vs real time. It is recommended to use the later one. Kinesis is the perfect service for that.\nEx: Twitter Trend\n![](https://raw.githubusercontent.com/JaminOne/jaminone.github.io/master/img/twittertrend.jpg)\n\n**Case study: COVID19 Outbreak heatmap**\nTraditional method: Regional governors send data to the FTP server. It's highly like to cause single point of failure. Aggregating data is delay and slow. AWS Kinesis can generate real time data but the payload size is limited. The global data may exceed the limit. Cloudfrond is compatible with this case, but it doesn't cache the data. There are lots of tradeoff when you look into this case.\n\n","tags":["ACA"]},{"title":"Project: OpenCV with Python to deal with image detection and process","url":"/2020/07/10/Project-OpenCV-with-Python-to-deal-with-image-detection-and-process/","content":"OpenCV is a powerful library in python when the objects you dealing are images and videos. Before the project starts, I need to install opencv-python, Numpy and Matplotlib in command line.\n```\npip install numpy\npip install matplotlib\npip install opencv-python\n```\nOpenCV's official [documentation page](https://docs.opencv.org/4.3.0/d2/d96/tutorial_py_table_of_contents_imgproc.html) \n\nPlease note that OpenCV is not compatible with jupyter notebook and spyder, therefore I used Visual Studio to do this task.\n\nThe final result can circle each coin with its value, and calculate total value on the image as well.\n![](https://raw.githubusercontent.com/JaminOne/jaminone.github.io/master/img/710opencv.jpg)\n\nMy code is below:\n```\nimport numpy as np\nimport cv2\nfrom matplotlib import pyplot as plt\n\n#function \ndef get_radius(circles):\n    radius = []\n    for j in circles[0,:]:\n        radius.append(j[2])\n    return radius\n\ndef av_pix(img,circles,size):\n    av_value = []\n    for coords in circles[0,:]:\n        col = np.mean(img[coords[1]-size:coords[1]+size,coords[0]-size:coords[0]+size])\n        #print(img[coords[1]-size:coords[1]+size,coords[0]-size:coords[0]+size])\n        av_value.append(col)\n    return av_value  \n\nimg = cv2.imread('E:\\\\NZStudy\\\\Python\\\\capstone_coins.png',0)\noriginal_image = cv2.imread('E:\\\\NZStudy\\\\Python\\\\capstone_coins.png',1)\n# convert BGR to RGB to be suitable for showing using matplotlib library\nimg = cv2.medianBlur(img,5)\ncimg = cv2.cvtColor(img,cv2.COLOR_GRAY2BGR)\nfont = cv2.FONT_HERSHEY_SIMPLEX \n\n#use houghcircles to detect coin\ncircles = cv2.HoughCircles(img,cv2.HOUGH_GRADIENT,1,130,\n                            param1=44,param2=75,minRadius=0,maxRadius=200)\nprint(circles)\n\ncircles = np.uint16(np.around(circles))\ncount = 1\nfor i in circles[0,:]:\n    # draw the outer circle\n    cv2.circle(original_image,(i[0],i[1]),i[2],(0,255,0),2)\n    # draw the center of the circle\n    cv2.circle(original_image,(i[0],i[1]),2,(0,0,255),3)\n    count +=1\n\n\nradii = get_radius(circles)\nprint(radii)\n\nbright_values = av_pix(img,circles,20)\nprint(bright_values)\n\nvalues = []\nfor a,b in zip(bright_values,radii):\n    if a > 150 and b > 106.5:\n        values.append(10)\n    elif a > 150 and b <= 106.5:\n        values.append(5)\n    elif a < 150 and b > 106.5:\n        values.append(2)\n    elif a < 150 and b < 106.5:\n        values.append(1)        \nprint(values)           \ncount_2 = 0\nfor i in circles[0,:]:\n    \n    cv2.putText(original_image, str(values[count_2]) + 'p',(i[0],i[1]), cv2.FONT_HERSHEY_SIMPLEX, 2, (0,0,0), 2)\n    count_2 += 1\ncv2.putText(original_image, 'ESTIMATED TOTAL VALUE: ' + str(sum(values)) + 'p', (200,100), cv2.FONT_HERSHEY_SIMPLEX, 1.3, 255)\n \noriginal_image = cv2.resize(original_image, (960, 540))\ncv2.imshow('detected circles',original_image)\ncv2.waitKey(0)\ncv2.destroyAllWindows()\n```","tags":["Machine Learning"]}]